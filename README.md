# Must-read papers on Natural Language Processing (NLP)

<br>

[![](https://sourcerer.io/fame/mmsamiei/aliakbarbadri/nlp-papers/images/0)](https://sourcerer.io/fame/mmsamiei/aliakbarbadri/nlp-papers/links/0)[![](https://sourcerer.io/fame/mmsamiei/aliakbarbadri/nlp-papers/images/1)](https://sourcerer.io/fame/mmsamiei/aliakbarbadri/nlp-papers/links/1)[![](https://sourcerer.io/fame/mmsamiei/aliakbarbadri/nlp-papers/images/2)](https://sourcerer.io/fame/mmsamiei/aliakbarbadri/nlp-papers/links/2)[![](https://sourcerer.io/fame/mmsamiei/aliakbarbadri/nlp-papers/images/3)](https://sourcerer.io/fame/mmsamiei/aliakbarbadri/nlp-papers/links/3)[![](https://sourcerer.io/fame/mmsamiei/aliakbarbadri/nlp-papers/images/4)](https://sourcerer.io/fame/mmsamiei/aliakbarbadri/nlp-papers/links/4)[![](https://sourcerer.io/fame/mmsamiei/aliakbarbadri/nlp-papers/images/5)](https://sourcerer.io/fame/mmsamiei/aliakbarbadri/nlp-papers/links/5)[![](https://sourcerer.io/fame/mmsamiei/aliakbarbadri/nlp-papers/images/6)](https://sourcerer.io/fame/mmsamiei/aliakbarbadri/nlp-papers/links/6)[![](https://sourcerer.io/fame/mmsamiei/aliakbarbadri/nlp-papers/images/7)](https://sourcerer.io/fame/mmsamiei/aliakbarbadri/nlp-papers/links/7)

# General
- [Beyond Accuracy: Behavioral Testing of NLP Models with CheckList](https://www.aclweb.org/anthology/2020.acl-main.442.pdf)



# [Transformers](https://huggingface.co/transformers/index.html#contents)
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
- [GPT: Improving Language Understanding by Generative Pre-Training](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)
- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)
- [GPT-2: Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
- [Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context](https://arxiv.org/abs/1901.02860)
- [XLNet: Generalized Autoregressive Pretraining for Language Understanding](https://arxiv.org/abs/1906.08237)
- [XLM: Cross-lingual Language Model Pretraining](https://arxiv.org/abs/1901.07291)
- [RoBERTa: Robustly Optimized BERT Pretraining Approach](https://arxiv.org/abs/1907.11692)
- [DistilBERT: DistilBERT, a distilled version of BERT: smaller, faster, cheaper, and lighter](https://arxiv.org/abs/1910.01108)
- [CTRL: A Conditional Transformer Language Model for Controllable Generation](https://www.github.com/salesforce/ctrl)
- [CamemBERT: a Tasty French Language Model](https://arxiv.org/abs/1911.03894)
- [ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://arxiv.org/abs/1909.11942)
- [T5: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/abs/1910.10683)
- [XLM-RoBERTa: Unsupervised Cross-lingual Representation Learning at Scale](https://arxiv.org/abs/1911.02116)
- [MMBT: Supervised Multimodal Bitransformers for Classifying Images and Text](https://arxiv.org/pdf/1909.02950.pdf)
- [FlauBERT: Unsupervised Language Model Pre-training for French](https://arxiv.org/abs/1912.05372)
- [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/pdf/1910.13461.pdf)
- [ELECTRA: Pre-training text encoders as discriminators rather than generators](https://arxiv.org/abs/2003.10555)
- [DialoGPT: Large-Scale Generative Pre-training for Conversational Response Generation](https://arxiv.org/abs/1911.00536)
- [Reformer: The Efficient Transformer](https://arxiv.org/abs/2001.04451)
- [Longformer: The Long-Document Transformer](https://arxiv.org/abs/2004.05150)

# GAN and VAE
- [SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient](https://arxiv.org/abs/1609.05473)
- [Generating Sentences from a Continuous Space](https://arxiv.org/abs/1511.06349)
- [Toward Controlled Generation of Text](https://arxiv.org/abs/1703.00955)
- [Adversarially Regularized Autoencoders](https://arxiv.org/abs/1706.04223)

# Dialouge Systems and Chatbots
- [A Neural Conversational Model](https://arxiv.org/abs/1506.05869)
- [A Knowledge-Grounded Neural Conversation Model](https://arxiv.org/abs/1702.01932)
- [Neural Approaches to Conversational AI](https://arxiv.org/abs/1809.08267)
- [Wizard of Wikipedia: Knowledge-Powered Conversational agents](https://arxiv.org/abs/1811.01241)
- [The Dialogue Dodecathlon: Open-Domain Knowledge and Image Grounded Conversational Agents](https://arxiv.org/abs/1911.03768)

<br>

# Thanks to
- [Danial Alihosseini](https://github.com/Danial-Alh)
- [Mohammad Mahdi Samiei](https://github.com/mmsamiei)
