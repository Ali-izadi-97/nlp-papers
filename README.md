**NLP must-read Papers List**

# General
- [Beyond Accuracy: Behavioral Testing of NLP Models with CheckList](https://www.aclweb.org/anthology/2020.acl-main.442.pdf)



# [Transformers](https://huggingface.co/transformers/index.html#contents)

- [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
- [GPT: Improving Language Understanding by Generative Pre-Training](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)
- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)
- [GPT-2: Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
- [Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context](https://arxiv.org/abs/1901.02860)
- [XLNet: Generalized Autoregressive Pretraining for Language Understanding](https://arxiv.org/abs/1906.08237)
- [XLM: Cross-lingual Language Model Pretraining](https://arxiv.org/abs/1901.07291)
- [RoBERTa: Robustly Optimized BERT Pretraining Approach](https://arxiv.org/abs/1907.11692)
- [DistilBERT: DistilBERT, a distilled version of BERT: smaller, faster, cheaper, and lighter](https://arxiv.org/abs/1910.01108)
- [CTRL: A Conditional Transformer Language Model for Controllable Generation](https://www.github.com/salesforce/ctrl)
- [CamemBERT: a Tasty French Language Model](https://arxiv.org/abs/1911.03894)
- [ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://arxiv.org/abs/1909.11942)
- [T5: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/abs/1910.10683)
- [XLM-RoBERTa: Unsupervised Cross-lingual Representation Learning at Scale](https://arxiv.org/abs/1911.02116)
- [MMBT: Supervised Multimodal Bitransformers for Classifying Images and Text](https://arxiv.org/pdf/1909.02950.pdf)
- [FlauBERT: Unsupervised Language Model Pre-training for French](https://arxiv.org/abs/1912.05372)
- [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/pdf/1910.13461.pdf)
- [ELECTRA: Pre-training text encoders as discriminators rather than generators](https://arxiv.org/abs/2003.10555)
- [DialoGPT: Large-Scale Generative Pre-training for Conversational Response Generation](https://arxiv.org/abs/1911.00536)
- [Reformer: The Efficient Transformer](https://arxiv.org/abs/2001.04451)
- [Longformer: The Long-Document Transformer](https://arxiv.org/abs/2004.05150)

<br>

# [GAN and VAE](https://huggingface.co/transformers/index.html#contents)

- [SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient](https://arxiv.org/abs/1609.05473)
- [Generating Sentences from a Continuous Space](https://arxiv.org/abs/1511.06349)
- [Toward Controlled Generation of Text](https://arxiv.org/abs/1703.00955)
- [Adversarially Regularized Autoencoders](https://arxiv.org/abs/1706.04223)

# Thanks to:
